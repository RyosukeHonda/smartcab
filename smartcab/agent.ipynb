{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('red', None, 'left', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'right', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', None, 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'left', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'right', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'left', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'right', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'right', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'left', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'right', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', None, 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'forward', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'right', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, 'left', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'left', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', None, 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'right', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'left', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', None, 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, None, 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, 'right', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'forward', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'forward', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, 'right', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'left', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'forward', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'forward', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'forward', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'forward', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'right', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'forward', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, 'left', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'left', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'right', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'right', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'right', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'forward', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'left', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'forward', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'forward', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'right', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'forward', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', None, None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'left', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', None, 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, 'forward', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, None, None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', None, 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'left', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'forward', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'right', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'right', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', None, 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', None, 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', None, 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'left', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'right', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'right', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'right', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'right', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, 'right', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'right', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'right', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'forward', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', None, 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', None, 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'forward', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', None, None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, 'left', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', None, 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', None, 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'left', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, 'right', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', None, None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'forward', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'forward', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, 'forward', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', None, 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'right', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'right', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'forward', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', None, 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'forward', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, None, 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'forward', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'left', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'forward', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'right', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'forward', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'left', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'left', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', None, 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'left', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, 'forward', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', None, None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'right', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', None, 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'left', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'left', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'left', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'left', 'left', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'left', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'forward', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', None, None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'right', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'right', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, 'forward', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'left', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, None, None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, 'forward', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, None, 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'forward', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, None, 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'right', None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'left', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', None, 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'left', 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'left', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', None, None): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'forward', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'right', 'left', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'forward', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', None, None, 'forward'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', None, None, 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'forward', 'left', 'right'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('green', 'right', 'left', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'left', 'right', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}, ('red', 'forward', 'forward', 'left'): {'forward': 0, 'right': 0, None: 0, 'left': 0}}\n",
      "Simulator.__init__(): Error initializing GUI objects; display disabled.\n",
      "error: \n",
      "Simulator.run(): Trial 0\n",
      "Environment.reset(): Trial set up with start = (4, 5), destination = (2, 1), deadline = 30\n",
      "RoutePlanner.route_to(): destination = (2, 1)\n",
      "Old_TIME 0\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:0\n",
      "RANDOM ACTION\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:0\n",
      "1.0 0.0322580645161\n",
      "LearningAgent.update(): deadline = 30, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 1\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:1\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:1\n",
      "0.5 0.0333333333333\n",
      "LearningAgent.update(): deadline = 29, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 2\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:2\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': 0}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 3.0\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:2\n",
      "0.333333333333 0.0344827586207\n",
      "LearningAgent.update(): deadline = 28, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 3\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:3\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': -0.3333333333333333}\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 5.0\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:3\n",
      "0.25 0.0357142857143\n",
      "LearningAgent.update(): deadline = 27, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 4\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:4\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': 0}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 4.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:4\n",
      "0.2 0.037037037037\n",
      "LearningAgent.update(): deadline = 26, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 5\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:5\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': 0}\n",
      "forward\n",
      "REWARD IS: -0.5\n",
      "Total Reward 4.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:5\n",
      "0.166666666667 0.0384615384615\n",
      "LearningAgent.update(): deadline = 25, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -0.5\n",
      "New_time 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 6\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:6\n",
      "{'forward': -0.08333333333333333, 'right': 0, None: 0, 'left': 0}\n",
      "left\n",
      "REWARD IS: -0.5\n",
      "Total Reward 3.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:6\n",
      "0.142857142857 0.04\n",
      "LearningAgent.update(): deadline = 24, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -0.5\n",
      "New_time 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 7\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:7\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': 0}\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 5.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:7\n",
      "0.125 0.0416666666667\n",
      "LearningAgent.update(): deadline = 23, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 8\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:8\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 7.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:8\n",
      "0.111111111111 0.0434782608696\n",
      "LearningAgent.update(): deadline = 22, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 9\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:9\n",
      "{'forward': -0.08333333333333333, 'right': 0, None: 0, 'left': -0.07142857142857142}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 7.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:9\n",
      "0.1 0.0454545454545\n",
      "LearningAgent.update(): deadline = 21, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 10\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:10\n",
      "{'forward': -0.08333333333333333, 'right': 0, None: 0.0, 'left': -0.07142857142857142}\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 9.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:10\n",
      "0.0909090909091 0.047619047619\n",
      "LearningAgent.update(): deadline = 20, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 11\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:11\n",
      "forward\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "REWARD IS: 12.0\n",
      "Total Reward 21.5\n",
      "The new state is: ('green', None, None, None)\n",
      "t:11\n",
      "0.0833333333333 0.05\n",
      "LearningAgent.update(): deadline = 19, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 12.0\n",
      "New_time 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Simulator.run(): Trial 1\n",
      "Environment.reset(): Trial set up with start = (5, 4), destination = (8, 1), deadline = 30\n",
      "RoutePlanner.route_to(): destination = (8, 1)\n",
      "Old_TIME 0\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:0\n",
      "RANDOM ACTION\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:0\n",
      "1.0 0.0322580645161\n",
      "LearningAgent.update(): deadline = 30, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 1\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:1\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('red', 'forward', 'left', None)\n",
      "t:1\n",
      "0.5 0.0333333333333\n",
      "LearningAgent.update(): deadline = 29, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 2\n",
      "The current state is: ('red', 'forward', 'left', None)\n",
      "t:2\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': 0}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 3.0\n",
      "The new state is: ('red', 'forward', 'left', None)\n",
      "t:2\n",
      "0.333333333333 0.0344827586207\n",
      "LearningAgent.update(): deadline = 28, inputs = {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 3\n",
      "The current state is: ('red', 'forward', 'left', None)\n",
      "t:3\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': -0.3333333333333333}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 2.5\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:3\n",
      "0.25 0.0357142857143\n",
      "LearningAgent.update(): deadline = 27, inputs = {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 4\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:4\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': 0}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 2.5\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:4\n",
      "0.2 0.037037037037\n",
      "LearningAgent.update(): deadline = 26, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 5\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:5\n",
      "{'forward': 0, 'right': 0, None: 0.0, 'left': 0}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 2.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:5\n",
      "0.166666666667 0.0384615384615\n",
      "LearningAgent.update(): deadline = 25, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 6\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:6\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('red', 'right', 'left', None)\n",
      "t:6\n",
      "0.142857142857 0.04\n",
      "LearningAgent.update(): deadline = 24, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 7\n",
      "The current state is: ('red', 'right', 'left', None)\n",
      "t:7\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': 0}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('red', 'right', 'left', None)\n",
      "t:7\n",
      "0.125 0.0416666666667\n",
      "LearningAgent.update(): deadline = 23, inputs = {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 8\n",
      "The current state is: ('red', 'right', 'left', None)\n",
      "t:8\n",
      "{'forward': 0, 'right': 0, None: 0.0, 'left': 0}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 3.0\n",
      "The new state is: ('red', 'right', 'left', None)\n",
      "t:8\n",
      "0.111111111111 0.0434782608696\n",
      "LearningAgent.update(): deadline = 22, inputs = {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 9\n",
      "The current state is: ('red', 'right', 'left', None)\n",
      "t:9\n",
      "{'forward': 0, 'right': 0, None: 0.0, 'left': -0.1111111111111111}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 3.0\n",
      "The new state is: ('red', 'right', 'left', None)\n",
      "t:9\n",
      "0.1 0.0454545454545\n",
      "LearningAgent.update(): deadline = 21, inputs = {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 10\n",
      "The current state is: ('green', 'right', 'left', None)\n",
      "t:10\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': 0}\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 5.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:10\n",
      "0.0909090909091 0.047619047619\n",
      "LearningAgent.update(): deadline = 20, inputs = {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 11\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:11\n",
      "{'forward': 0, 'right': -0.1, None: 0, 'left': 0}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:11\n",
      "0.0833333333333 0.05\n",
      "LearningAgent.update(): deadline = 19, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 12\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:12\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 6.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:12\n",
      "0.0769230769231 0.0526315789474\n",
      "LearningAgent.update(): deadline = 18, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 13\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:13\n",
      "{'forward': 0, 'right': -0.1, None: 0, 'left': -0.08333333333333333}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 5.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:13\n",
      "0.0714285714286 0.0555555555556\n",
      "LearningAgent.update(): deadline = 17, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 13\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 14\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:14\n",
      "{'forward': 0, 'right': -0.1, None: 0, 'left': -0.1488095238095238}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 4.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:14\n",
      "0.0666666666667 0.0588235294118\n",
      "LearningAgent.update(): deadline = 16, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 14\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 15\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:15\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 6.5\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:15\n",
      "0.0625 0.0625\n",
      "LearningAgent.update(): deadline = 15, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 15\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 16\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:16\n",
      "RANDOM ACTION\n",
      "REWARD IS: -1.0\n",
      "Total Reward 5.5\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:16\n",
      "0.0588235294118 0.0666666666667\n",
      "LearningAgent.update(): deadline = 14, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 16\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 17\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:17\n",
      "{'forward': -0.058823529411764705, 'right': -0.0763125763125763, None: 0.0, 'left': 0}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 5.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:17\n",
      "0.0555555555556 0.0714285714286\n",
      "LearningAgent.update(): deadline = 13, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 18\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:18\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 7.0\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:18\n",
      "0.0526315789474 0.0769230769231\n",
      "LearningAgent.update(): deadline = 12, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 18\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 19\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:19\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 9.0\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:19\n",
      "0.05 0.0833333333333\n",
      "LearningAgent.update(): deadline = 11, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 19\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 20\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:20\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 11.0\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:20\n",
      "0.047619047619 0.0909090909091\n",
      "LearningAgent.update(): deadline = 10, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 20\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 21\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:21\n",
      "{'forward': -0.058823529411764705, 'right': -0.09550458359982168, None: 0.0, 'left': 0}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 10.0\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:21\n",
      "0.0454545454545 0.1\n",
      "LearningAgent.update(): deadline = 9, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 21\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 22\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:22\n",
      "{'forward': -0.10160427807486631, 'right': -0.09550458359982168, None: 0.0, 'left': 0}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 9.0\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:22\n",
      "0.0434782608696 0.111111111111\n",
      "LearningAgent.update(): deadline = 8, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 22\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 23\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:23\n",
      "{'forward': -0.14066496163682865, 'right': -0.09550458359982168, None: 0.0, 'left': 0}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 8.0\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:23\n",
      "0.0416666666667 0.125\n",
      "LearningAgent.update(): deadline = 7, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 23\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 24\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:24\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 10.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:24\n",
      "0.04 0.142857142857\n",
      "LearningAgent.update(): deadline = 6, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 24\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 25\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:25\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 12.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:25\n",
      "0.0384615384615 0.166666666667\n",
      "LearningAgent.update(): deadline = 5, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 25\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 26\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:26\n",
      "{'forward': 0, 'right': -0.1256454248366013, None: 0, 'left': -0.1488095238095238}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 11.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:26\n",
      "0.037037037037 0.2\n",
      "LearningAgent.update(): deadline = 4, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 26\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 27\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:27\n",
      "{'forward': -0.037037037037037035, 'right': -0.1256454248366013, None: 0, 'left': -0.1488095238095238}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 10.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:27\n",
      "0.0357142857143 0.25\n",
      "LearningAgent.update(): deadline = 3, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 27\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 28\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:28\n",
      "forward\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "REWARD IS: 12.0\n",
      "Total Reward 22.0\n",
      "The new state is: ('red', None, None, 'forward')\n",
      "t:28\n",
      "0.0344827586207 0.333333333333\n",
      "LearningAgent.update(): deadline = 2, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 12.0\n",
      "New_time 28\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Simulator.run(): Trial 2\n",
      "Environment.reset(): Trial set up with start = (6, 3), destination = (2, 6), deadline = 35\n",
      "RoutePlanner.route_to(): destination = (2, 6)\n",
      "Old_TIME 0\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:0\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:0\n",
      "1.0 0.0277777777778\n",
      "LearningAgent.update(): deadline = 35, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 1\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:1\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:1\n",
      "0.5 0.0285714285714\n",
      "LearningAgent.update(): deadline = 34, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 2\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:2\n",
      "{'forward': -0.07142857142857142, 'right': -0.1256454248366013, None: 0, 'left': -0.1488095238095238}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 3.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:2\n",
      "0.333333333333 0.0294117647059\n",
      "LearningAgent.update(): deadline = 33, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 3\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:3\n",
      "{'forward': -0.14066496163682865, 'right': -0.09550458359982168, None: 0.0, 'left': -0.041666666666666664}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 2.5\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:3\n",
      "0.25 0.030303030303\n",
      "LearningAgent.update(): deadline = 32, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 4\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:4\n",
      "{'forward': -0.3554987212276215, 'right': -0.09550458359982168, None: 0.0, 'left': -0.041666666666666664}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 2.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:4\n",
      "0.2 0.03125\n",
      "LearningAgent.update(): deadline = 31, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 5\n",
      "The current state is: ('green', 'right', None, 'left')\n",
      "t:5\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': 0}\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:5\n",
      "0.166666666667 0.0322580645161\n",
      "LearningAgent.update(): deadline = 30, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, action = right, reward = 2.0\n",
      "New_time 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 6\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:6\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 6.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:6\n",
      "0.142857142857 0.0333333333333\n",
      "LearningAgent.update(): deadline = 29, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 7\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:7\n",
      "{'forward': -0.07142857142857142, 'right': -0.24617170479302833, None: 0, 'left': -0.1488095238095238}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 5.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:7\n",
      "0.125 0.0344827586207\n",
      "LearningAgent.update(): deadline = 28, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 8\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:8\n",
      "{'forward': -0.1875, 'right': -0.24617170479302833, None: 0, 'left': -0.1488095238095238}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:8\n",
      "0.111111111111 0.0357142857143\n",
      "LearningAgent.update(): deadline = 27, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 9\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:9\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 6.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:9\n",
      "0.1 0.037037037037\n",
      "LearningAgent.update(): deadline = 26, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 10\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:10\n",
      "{'forward': -0.2777777777777778, 'right': -0.24617170479302833, None: 0, 'left': -0.1488095238095238}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 5.5\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:10\n",
      "0.0909090909091 0.0384615384615\n",
      "LearningAgent.update(): deadline = 25, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 11\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:11\n",
      "{'forward': -0.3554987212276215, 'right': -0.16894739578973658, None: 0.0, 'left': -0.041666666666666664}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 4.5\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:11\n",
      "0.0833333333333 0.04\n",
      "LearningAgent.update(): deadline = 24, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 12\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:12\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 6.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:12\n",
      "0.0769230769231 0.0416666666667\n",
      "LearningAgent.update(): deadline = 23, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 13\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:13\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 8.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:13\n",
      "0.0714285714286 0.0434782608696\n",
      "LearningAgent.update(): deadline = 22, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 13\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 14\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:14\n",
      "{'forward': -0.2777777777777778, 'right': -0.2692470043572985, None: 0, 'left': -0.1488095238095238}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 7.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:14\n",
      "0.0666666666667 0.0454545454545\n",
      "LearningAgent.update(): deadline = 21, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 14\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 15\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:15\n",
      "{'forward': -0.2777777777777778, 'right': -0.2692470043572985, None: 0, 'left': -0.20555555555555555}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 7.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:15\n",
      "0.0625 0.047619047619\n",
      "LearningAgent.update(): deadline = 20, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 15\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 16\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:16\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 9.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:16\n",
      "0.0588235294118 0.05\n",
      "LearningAgent.update(): deadline = 19, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 16\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 17\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:17\n",
      "{'forward': -0.2777777777777778, 'right': -0.2692470043572985, None: 0.0, 'left': -0.20555555555555555}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 9.0\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:17\n",
      "0.0555555555556 0.0526315789474\n",
      "LearningAgent.update(): deadline = 18, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 18\n",
      "The current state is: ('red', 'right', None, 'right')\n",
      "t:18\n",
      "{'forward': 0, 'right': 0, None: 0, 'left': 0}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 9.0\n",
      "The new state is: ('red', 'right', None, 'right')\n",
      "t:18\n",
      "0.0526315789474 0.0555555555556\n",
      "LearningAgent.update(): deadline = 17, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, action = None, reward = 0.0\n",
      "New_time 18\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 19\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:19\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 11.0\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:19\n",
      "0.05 0.0588235294118\n",
      "LearningAgent.update(): deadline = 16, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 19\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 20\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:20\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 13.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:20\n",
      "0.047619047619 0.0625\n",
      "LearningAgent.update(): deadline = 15, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 20\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 21\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:21\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 15.0\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:21\n",
      "0.0454545454545 0.0666666666667\n",
      "LearningAgent.update(): deadline = 14, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 21\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 22\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:22\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 17.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:22\n",
      "0.0434782608696 0.0714285714286\n",
      "LearningAgent.update(): deadline = 13, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 22\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 23\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:23\n",
      "{'forward': -0.2777777777777778, 'right': -0.2759702078548591, None: 0.0, 'left': -0.20555555555555555}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 16.5\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:23\n",
      "0.0416666666667 0.0769230769231\n",
      "LearningAgent.update(): deadline = 12, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 23\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 24\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:24\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 18.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:24\n",
      "0.04 0.0833333333333\n",
      "LearningAgent.update(): deadline = 11, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 24\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 25\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:25\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 20.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:25\n",
      "0.0384615384615 0.0909090909091\n",
      "LearningAgent.update(): deadline = 10, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 25\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 26\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:26\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 22.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:26\n",
      "0.037037037037 0.1\n",
      "LearningAgent.update(): deadline = 9, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 26\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 27\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:27\n",
      "forward\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "REWARD IS: 12.0\n",
      "Total Reward 34.5\n",
      "The new state is: ('green', None, None, None)\n",
      "t:27\n",
      "0.0357142857143 0.111111111111\n",
      "LearningAgent.update(): deadline = 8, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 12.0\n",
      "New_time 27\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Simulator.run(): Trial 3\n",
      "Environment.reset(): Trial set up with start = (4, 5), destination = (2, 1), deadline = 30\n",
      "RoutePlanner.route_to(): destination = (2, 1)\n",
      "Old_TIME 0\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:0\n",
      "RANDOM ACTION\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:0\n",
      "1.0 0.0322580645161\n",
      "LearningAgent.update(): deadline = 30, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 1\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:1\n",
      "{'forward': -0.2777777777777778, 'right': -0.27861692103343294, None: 0.0, 'left': -0.20555555555555555}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 1.5\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:1\n",
      "0.5 0.0333333333333\n",
      "LearningAgent.update(): deadline = 29, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 2\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:2\n",
      "{'forward': -0.4092071611253197, 'right': -0.16894739578973658, None: 0.0, 'left': -0.041666666666666664}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 1.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:2\n",
      "0.333333333333 0.0344827586207\n",
      "LearningAgent.update(): deadline = 28, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 3\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:3\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 3.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:3\n",
      "0.25 0.0357142857143\n",
      "LearningAgent.update(): deadline = 27, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 4\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:4\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 5.0\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:4\n",
      "0.2 0.037037037037\n",
      "LearningAgent.update(): deadline = 26, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 5\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:5\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 7.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:5\n",
      "0.166666666667 0.0384615384615\n",
      "LearningAgent.update(): deadline = 25, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 6\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:6\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 9.0\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:6\n",
      "0.142857142857 0.04\n",
      "LearningAgent.update(): deadline = 24, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 7\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:7\n",
      "RANDOM ACTION\n",
      "REWARD IS: 2.0\n",
      "Total Reward 11.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:7\n",
      "0.125 0.0416666666667\n",
      "LearningAgent.update(): deadline = 23, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 8\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:8\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 13.0\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:8\n",
      "0.111111111111 0.0434782608696\n",
      "LearningAgent.update(): deadline = 22, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 9\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:9\n",
      "{'forward': -0.2777777777777778, 'right': -0.38930846051671647, None: 0.0, 'left': -0.20555555555555555}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 12.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:9\n",
      "0.1 0.0454545454545\n",
      "LearningAgent.update(): deadline = 21, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 10\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:10\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 14.5\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:10\n",
      "0.0909090909091 0.047619047619\n",
      "LearningAgent.update(): deadline = 20, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 11\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:11\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 16.5\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:11\n",
      "0.0833333333333 0.05\n",
      "LearningAgent.update(): deadline = 19, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 12\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:12\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 18.5\n",
      "The new state is: ('green', 'forward', None, 'right')\n",
      "t:12\n",
      "0.0769230769231 0.0526315789474\n",
      "LearningAgent.update(): deadline = 18, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 13\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:13\n",
      "RANDOM ACTION\n",
      "REWARD IS: -0.5\n",
      "Total Reward 18.0\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:13\n",
      "0.0714285714286 0.0555555555556\n",
      "LearningAgent.update(): deadline = 17, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -0.5\n",
      "New_time 13\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 14\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:14\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 20.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:14\n",
      "0.0666666666667 0.0588235294118\n",
      "LearningAgent.update(): deadline = 16, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 14\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 15\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:15\n",
      "right\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "REWARD IS: 12.0\n",
      "Total Reward 32.0\n",
      "The new state is: ('red', None, None, None)\n",
      "t:15\n",
      "0.0625 0.0625\n",
      "LearningAgent.update(): deadline = 15, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 12.0\n",
      "New_time 15\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Simulator.run(): Trial 4\n",
      "Environment.reset(): Trial set up with start = (2, 4), destination = (8, 1), deadline = 45\n",
      "RoutePlanner.route_to(): destination = (8, 1)\n",
      "Old_TIME 0\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:0\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:0\n",
      "1.0 0.0217391304348\n",
      "LearningAgent.update(): deadline = 45, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 1\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:1\n",
      "{'forward': -0.2777777777777778, 'right': -0.39281152492223315, None: 0.0, 'left': -0.20555555555555555}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 1.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:1\n",
      "0.5 0.0222222222222\n",
      "LearningAgent.update(): deadline = 44, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 2\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:2\n",
      "{'forward': -0.6388888888888888, 'right': -0.39281152492223315, None: 0.0, 'left': -0.20555555555555555}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 0.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:2\n",
      "0.333333333333 0.0227272727273\n",
      "LearningAgent.update(): deadline = 43, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 3\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:3\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:3\n",
      "0.25 0.0232558139535\n",
      "LearningAgent.update(): deadline = 42, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 4\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:4\n",
      "RANDOM ACTION\n",
      "REWARD IS: -0.5\n",
      "Total Reward 1.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:4\n",
      "0.2 0.0238095238095\n",
      "LearningAgent.update(): deadline = 41, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 5\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:5\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 3.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:5\n",
      "0.166666666667 0.0243902439024\n",
      "LearningAgent.update(): deadline = 40, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 6\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:6\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 5.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:6\n",
      "0.142857142857 0.025\n",
      "LearningAgent.update(): deadline = 39, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 7\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:7\n",
      "{'forward': -0.7592592592592593, 'right': -0.39281152492223315, None: 0.0, 'left': -0.20555555555555555}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 4.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:7\n",
      "0.125 0.025641025641\n",
      "LearningAgent.update(): deadline = 38, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 8\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:8\n",
      "{'forward': -0.7893518518518519, 'right': -0.39281152492223315, None: 0.0, 'left': -0.20555555555555555}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 3.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:8\n",
      "0.111111111111 0.0263157894737\n",
      "LearningAgent.update(): deadline = 37, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 9\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:9\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 5.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:9\n",
      "0.1 0.027027027027\n",
      "LearningAgent.update(): deadline = 36, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 10\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:10\n",
      "{'forward': -0.7893518518518519, 'right': -0.39281152492223315, None: 0.0, 'left': -0.2938271604938272}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 5.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:10\n",
      "0.0909090909091 0.0277777777778\n",
      "LearningAgent.update(): deadline = 35, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 11\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:11\n",
      "{'forward': -0.7893518518518519, 'right': -0.39281152492223315, None: 0.0, 'left': -0.2938271604938272}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 4.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:11\n",
      "0.0833333333333 0.0285714285714\n",
      "LearningAgent.update(): deadline = 34, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 12\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:12\n",
      "{'forward': -0.8069058641975309, 'right': -0.39281152492223315, None: 0.0, 'left': -0.2938271604938272}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 4.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:12\n",
      "0.0769230769231 0.0294117647059\n",
      "LearningAgent.update(): deadline = 33, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 13\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:13\n",
      "{'forward': -0.8069058641975309, 'right': -0.39281152492223315, None: 0.0, 'left': -0.2938271604938272}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 4.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:13\n",
      "0.0714285714286 0.030303030303\n",
      "LearningAgent.update(): deadline = 32, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 13\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 14\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:14\n",
      "{'forward': -0.8069058641975309, 'right': -0.39281152492223315, None: 0.0, 'left': -0.2938271604938272}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 4.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:14\n",
      "0.0666666666667 0.03125\n",
      "LearningAgent.update(): deadline = 31, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 14\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 15\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:15\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 6.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:15\n",
      "0.0625 0.0322580645161\n",
      "LearningAgent.update(): deadline = 30, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 15\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 16\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:16\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 8.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:16\n",
      "0.0588235294118 0.0333333333333\n",
      "LearningAgent.update(): deadline = 29, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 16\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 17\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:17\n",
      "{'forward': -0.8069058641975309, 'right': -0.39281152492223315, None: 0.0, 'left': -0.2938271604938272}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 7.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:17\n",
      "0.0555555555556 0.0344827586207\n",
      "LearningAgent.update(): deadline = 28, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 18\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:18\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 9.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:18\n",
      "0.0526315789474 0.0357142857143\n",
      "LearningAgent.update(): deadline = 27, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 18\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 19\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:19\n",
      "RANDOM ACTION\n",
      "REWARD IS: -1.0\n",
      "Total Reward 8.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:19\n",
      "0.05 0.037037037037\n",
      "LearningAgent.update(): deadline = 26, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 19\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 20\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:20\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 10.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:20\n",
      "0.047619047619 0.0384615384615\n",
      "LearningAgent.update(): deadline = 25, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 20\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 21\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:21\n",
      "forward\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "REWARD IS: 12.0\n",
      "Total Reward 22.5\n",
      "The new state is: ('green', None, None, None)\n",
      "t:21\n",
      "0.0454545454545 0.04\n",
      "LearningAgent.update(): deadline = 24, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 12.0\n",
      "New_time 21\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Simulator.run(): Trial 5\n",
      "Environment.reset(): Trial set up with start = (2, 3), destination = (6, 1), deadline = 30\n",
      "RoutePlanner.route_to(): destination = (6, 1)\n",
      "Old_TIME 0\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:0\n",
      "RANDOM ACTION\n",
      "REWARD IS: 0.0\n",
      "Total Reward 0.0\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:0\n",
      "1.0 0.0322580645161\n",
      "LearningAgent.update(): deadline = 30, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 1\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:1\n",
      "RANDOM ACTION\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:1\n",
      "0.5 0.0333333333333\n",
      "LearningAgent.update(): deadline = 29, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 2\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:2\n",
      "{'forward': -0.8267516503772291, 'right': -0.39281152492223315, None: 0.0, 'left': -0.2938271604938272}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 1.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:2\n",
      "0.333333333333 0.0344827586207\n",
      "LearningAgent.update(): deadline = 28, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 3\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:3\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 3.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:3\n",
      "0.25 0.0357142857143\n",
      "LearningAgent.update(): deadline = 27, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 4\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:4\n",
      "{'forward': -0.8267516503772291, 'right': -0.39281152492223315, None: 0.0, 'left': -0.5292181069958848}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:4\n",
      "0.2 0.037037037037\n",
      "LearningAgent.update(): deadline = 26, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 5\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:5\n",
      "{'forward': -0.8267516503772291, 'right': -0.39281152492223315, None: 0.0, 'left': -0.6233744855967078}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 1.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:5\n",
      "0.166666666667 0.0384615384615\n",
      "LearningAgent.update(): deadline = 25, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 6\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:6\n",
      "RANDOM ACTION\n",
      "REWARD IS: -0.5\n",
      "Total Reward 0.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:6\n",
      "0.142857142857 0.04\n",
      "LearningAgent.update(): deadline = 24, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 7\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:7\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:7\n",
      "0.125 0.0416666666667\n",
      "LearningAgent.update(): deadline = 23, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 8\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:8\n",
      "{'forward': -0.8267516503772291, 'right': -0.39281152492223315, None: 0.0, 'left': -0.6861454046639232}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 2.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:8\n",
      "0.111111111111 0.0434782608696\n",
      "LearningAgent.update(): deadline = 22, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 9\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:9\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 4.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:9\n",
      "0.1 0.0454545454545\n",
      "LearningAgent.update(): deadline = 21, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 10\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:10\n",
      "RANDOM ACTION\n",
      "REWARD IS: 0.0\n",
      "Total Reward 4.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:10\n",
      "0.0909090909091 0.047619047619\n",
      "LearningAgent.update(): deadline = 20, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 11\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:11\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 6.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:11\n",
      "0.0833333333333 0.05\n",
      "LearningAgent.update(): deadline = 19, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 12\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:12\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 8.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:12\n",
      "0.0769230769231 0.0526315789474\n",
      "LearningAgent.update(): deadline = 18, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 13\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:13\n",
      "RANDOM ACTION\n",
      "REWARD IS: -0.5\n",
      "Total Reward 8.0\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:13\n",
      "0.0714285714286 0.0555555555556\n",
      "LearningAgent.update(): deadline = 17, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 13\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 14\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:14\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 10.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:14\n",
      "0.0666666666667 0.0588235294118\n",
      "LearningAgent.update(): deadline = 16, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 14\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 15\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:15\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 12.0\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:15\n",
      "0.0625 0.0625\n",
      "LearningAgent.update(): deadline = 15, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 15\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 16\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:16\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 14.0\n",
      "The new state is: ('red', 'forward', None, 'left')\n",
      "t:16\n",
      "0.0588235294118 0.0666666666667\n",
      "LearningAgent.update(): deadline = 14, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 16\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 17\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:17\n",
      "{'forward': -0.8267516503772291, 'right': -0.39281152492223315, None: 0.0, 'left': -0.6861454046639232}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 13.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:17\n",
      "0.0555555555556 0.0714285714286\n",
      "LearningAgent.update(): deadline = 13, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 18\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:18\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 15.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:18\n",
      "0.0526315789474 0.0769230769231\n",
      "LearningAgent.update(): deadline = 12, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 18\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 19\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:19\n",
      "RANDOM ACTION\n",
      "REWARD IS: 2.0\n",
      "Total Reward 17.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:19\n",
      "0.05 0.0833333333333\n",
      "LearningAgent.update(): deadline = 11, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 19\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 20\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:20\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 19.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:20\n",
      "0.047619047619 0.0909090909091\n",
      "LearningAgent.update(): deadline = 10, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 20\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 21\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:21\n",
      "forward\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "REWARD IS: 12.0\n",
      "Total Reward 31.5\n",
      "The new state is: ('red', None, None, None)\n",
      "t:21\n",
      "0.0454545454545 0.1\n",
      "LearningAgent.update(): deadline = 9, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 12.0\n",
      "New_time 21\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Simulator.run(): Trial 6\n",
      "Environment.reset(): Trial set up with start = (4, 6), destination = (8, 3), deadline = 35\n",
      "RoutePlanner.route_to(): destination = (8, 3)\n",
      "Old_TIME 0\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:0\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:0\n",
      "1.0 0.0277777777778\n",
      "LearningAgent.update(): deadline = 35, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 1\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:1\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:1\n",
      "0.5 0.0285714285714\n",
      "LearningAgent.update(): deadline = 34, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 2\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:2\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 6.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:2\n",
      "0.333333333333 0.0294117647059\n",
      "LearningAgent.update(): deadline = 33, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 3\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:3\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 8.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:3\n",
      "0.25 0.030303030303\n",
      "LearningAgent.update(): deadline = 32, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 4\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:4\n",
      "{'forward': -0.8267516503772291, 'right': -0.3894520876324862, None: 0.0, 'left': -0.6861454046639232}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 7.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:4\n",
      "0.2 0.03125\n",
      "LearningAgent.update(): deadline = 31, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 5\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:5\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 9.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:5\n",
      "0.166666666667 0.0322580645161\n",
      "LearningAgent.update(): deadline = 30, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 6\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:6\n",
      "{'forward': -0.4092071611253197, 'right': -0.26277548520638033, None: 0.0, 'left': -0.041666666666666664}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 8.5\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:6\n",
      "0.142857142857 0.0333333333333\n",
      "LearningAgent.update(): deadline = 29, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 7\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:7\n",
      "{'forward': -0.4092071611253197, 'right': -0.26277548520638033, None: 0.0, 'left': -0.17857142857142855}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 8.5\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:7\n",
      "0.125 0.0344827586207\n",
      "LearningAgent.update(): deadline = 28, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 8\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:8\n",
      "{'forward': -0.4092071611253197, 'right': -0.26277548520638033, None: 0.0, 'left': -0.17857142857142855}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 8.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:8\n",
      "0.111111111111 0.0357142857143\n",
      "LearningAgent.update(): deadline = 27, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 9\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:9\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 10.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:9\n",
      "0.1 0.037037037037\n",
      "LearningAgent.update(): deadline = 26, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 10\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:10\n",
      "{'forward': -0.8267516503772291, 'right': -0.399061670105989, None: 0.0, 'left': -0.6861454046639232}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 10.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:10\n",
      "0.0909090909091 0.0384615384615\n",
      "LearningAgent.update(): deadline = 25, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 11\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:11\n",
      "{'forward': -0.8267516503772291, 'right': -0.399061670105989, None: 0.0, 'left': -0.6861454046639232}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 10.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:11\n",
      "0.0833333333333 0.04\n",
      "LearningAgent.update(): deadline = 24, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 12\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:12\n",
      "forward\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "REWARD IS: 12.0\n",
      "Total Reward 22.0\n",
      "The new state is: ('green', None, None, None)\n",
      "t:12\n",
      "0.0769230769231 0.0416666666667\n",
      "LearningAgent.update(): deadline = 23, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 12.0\n",
      "New_time 12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Simulator.run(): Trial 7\n",
      "Environment.reset(): Trial set up with start = (6, 1), destination = (5, 4), deadline = 20\n",
      "RoutePlanner.route_to(): destination = (5, 4)\n",
      "Old_TIME 0\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:0\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:0\n",
      "1.0 0.047619047619\n",
      "LearningAgent.update(): deadline = 20, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 1\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:1\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:1\n",
      "0.5 0.05\n",
      "LearningAgent.update(): deadline = 19, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 2\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:2\n",
      "{'forward': -0.4092071611253197, 'right': -0.28913376462789364, None: 0.0, 'left': -0.17857142857142855}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 3.0\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:2\n",
      "0.333333333333 0.0526315789474\n",
      "LearningAgent.update(): deadline = 18, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 3\n",
      "The current state is: ('red', 'left', None, None)\n",
      "t:3\n",
      "{'forward': -0.4092071611253197, 'right': -0.28913376462789364, None: 0.0, 'left': -0.45238095238095233}\n",
      "right\n",
      "REWARD IS: -0.5\n",
      "Total Reward 2.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:3\n",
      "0.25 0.0555555555556\n",
      "LearningAgent.update(): deadline = 17, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = -0.5\n",
      "New_time 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 4\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:4\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 4.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:4\n",
      "0.2 0.0588235294118\n",
      "LearningAgent.update(): deadline = 16, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 5\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:5\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 6.5\n",
      "The new state is: ('red', 'right', None, None)\n",
      "t:5\n",
      "0.166666666667 0.0625\n",
      "LearningAgent.update(): deadline = 15, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 6\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:6\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 8.5\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:6\n",
      "0.142857142857 0.0666666666667\n",
      "LearningAgent.update(): deadline = 14, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 7\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:7\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 10.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:7\n",
      "0.125 0.0714285714286\n",
      "LearningAgent.update(): deadline = 13, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 8\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:8\n",
      "forward\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "REWARD IS: 12.0\n",
      "Total Reward 22.5\n",
      "The new state is: ('red', None, None, None)\n",
      "t:8\n",
      "0.111111111111 0.0769230769231\n",
      "LearningAgent.update(): deadline = 12, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 12.0\n",
      "New_time 8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Simulator.run(): Trial 8\n",
      "Environment.reset(): Trial set up with start = (5, 6), destination = (7, 2), deadline = 30\n",
      "RoutePlanner.route_to(): destination = (7, 2)\n",
      "Old_TIME 0\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:0\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:0\n",
      "1.0 0.0322580645161\n",
      "LearningAgent.update(): deadline = 30, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 1\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:1\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('green', 'left', None, None)\n",
      "t:1\n",
      "0.5 0.0333333333333\n",
      "LearningAgent.update(): deadline = 29, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 2\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:2\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 6.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:2\n",
      "0.333333333333 0.0344827586207\n",
      "LearningAgent.update(): deadline = 28, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 3\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:3\n",
      "{'forward': -0.8267516503772291, 'right': -0.399061670105989, None: 0.0, 'left': -0.6861454046639232}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 5.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:3\n",
      "0.25 0.0357142857143\n",
      "LearningAgent.update(): deadline = 27, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 4\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:4\n",
      "{'forward': -0.8267516503772291, 'right': -0.399061670105989, None: 0.0, 'left': -0.7646090534979424}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 4.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:4\n",
      "0.2 0.037037037037\n",
      "LearningAgent.update(): deadline = 26, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 5\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:5\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 6.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:5\n",
      "0.166666666667 0.0384615384615\n",
      "LearningAgent.update(): deadline = 25, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 6\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:6\n",
      "{'forward': -0.8267516503772291, 'right': -0.399061670105989, None: 0.0, 'left': -0.8116872427983539}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 6.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:6\n",
      "0.142857142857 0.04\n",
      "LearningAgent.update(): deadline = 24, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 7\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:7\n",
      "{'forward': -0.8267516503772291, 'right': -0.399061670105989, None: 0.0, 'left': -0.8116872427983539}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 5.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:7\n",
      "0.125 0.0416666666667\n",
      "LearningAgent.update(): deadline = 23, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 8\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:8\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 7.0\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:8\n",
      "0.111111111111 0.0434782608696\n",
      "LearningAgent.update(): deadline = 22, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Old_TIME 9\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:9\n",
      "forward\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "REWARD IS: 12.0\n",
      "Total Reward 19.0\n",
      "The new state is: ('green', None, None, None)\n",
      "t:9\n",
      "0.1 0.0454545454545\n",
      "LearningAgent.update(): deadline = 21, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 12.0\n",
      "New_time 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Simulator.run(): Trial 9\n",
      "Environment.reset(): Trial set up with start = (7, 5), destination = (2, 4), deadline = 30\n",
      "RoutePlanner.route_to(): destination = (2, 4)\n",
      "Old_TIME 0\n",
      "The current state is: ('red', 'right', None, None)\n",
      "t:0\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 2.0\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:0\n",
      "1.0 0.0322580645161\n",
      "LearningAgent.update(): deadline = 30, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "132     3     8.0        32        7\n",
      "133     4     7.5        31        7\n",
      "134     5     9.5        30        7\n",
      "135     6     8.5        29        7\n",
      "136     7     8.5        28        7\n",
      "137     8     8.0        27        7\n",
      "138     9    10.0        26        7\n",
      "139    10    10.0        25        7\n",
      "140    11    10.0        24        7\n",
      "141    12    22.0        23        7\n",
      "142     0     2.0        20        8\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "\n",
      "[162 rows x 4 columns]\n",
      "Old_TIME 1\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:1\n",
      "RANDOM ACTION\n",
      "REWARD IS: -0.5\n",
      "Total Reward 1.5\n",
      "The new state is: ('green', 'right', None, None)\n",
      "t:1\n",
      "0.5 0.0333333333333\n",
      "LearningAgent.update(): deadline = 29, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -0.5\n",
      "New_time 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "133     4     7.5        31        7\n",
      "134     5     9.5        30        7\n",
      "135     6     8.5        29        7\n",
      "136     7     8.5        28        7\n",
      "137     8     8.0        27        7\n",
      "138     9    10.0        26        7\n",
      "139    10    10.0        25        7\n",
      "140    11    10.0        24        7\n",
      "141    12    22.0        23        7\n",
      "142     0     2.0        20        8\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "\n",
      "[163 rows x 4 columns]\n",
      "Old_TIME 2\n",
      "The current state is: ('green', 'right', None, None)\n",
      "t:2\n",
      "right\n",
      "REWARD IS: 2.0\n",
      "Total Reward 3.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:2\n",
      "0.333333333333 0.0344827586207\n",
      "LearningAgent.update(): deadline = 28, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = right, reward = 2.0\n",
      "New_time 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "134     5     9.5        30        7\n",
      "135     6     8.5        29        7\n",
      "136     7     8.5        28        7\n",
      "137     8     8.0        27        7\n",
      "138     9    10.0        26        7\n",
      "139    10    10.0        25        7\n",
      "140    11    10.0        24        7\n",
      "141    12    22.0        23        7\n",
      "142     0     2.0        20        8\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "\n",
      "[164 rows x 4 columns]\n",
      "Old_TIME 3\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:3\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 5.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:3\n",
      "0.25 0.0357142857143\n",
      "LearningAgent.update(): deadline = 27, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "135     6     8.5        29        7\n",
      "136     7     8.5        28        7\n",
      "137     8     8.0        27        7\n",
      "138     9    10.0        26        7\n",
      "139    10    10.0        25        7\n",
      "140    11    10.0        24        7\n",
      "141    12    22.0        23        7\n",
      "142     0     2.0        20        8\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "\n",
      "[165 rows x 4 columns]\n",
      "Old_TIME 4\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:4\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 7.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:4\n",
      "0.2 0.037037037037\n",
      "LearningAgent.update(): deadline = 26, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "136     7     8.5        28        7\n",
      "137     8     8.0        27        7\n",
      "138     9    10.0        26        7\n",
      "139    10    10.0        25        7\n",
      "140    11    10.0        24        7\n",
      "141    12    22.0        23        7\n",
      "142     0     2.0        20        8\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "\n",
      "[166 rows x 4 columns]\n",
      "Old_TIME 5\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:5\n",
      "{'forward': -0.8267516503772291, 'right': -0.399061670105989, None: 0.0, 'left': -0.8352263374485597}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 6.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:5\n",
      "0.166666666667 0.0384615384615\n",
      "LearningAgent.update(): deadline = 25, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "137     8     8.0        27        7\n",
      "138     9    10.0        26        7\n",
      "139    10    10.0        25        7\n",
      "140    11    10.0        24        7\n",
      "141    12    22.0        23        7\n",
      "142     0     2.0        20        8\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "\n",
      "[167 rows x 4 columns]\n",
      "Old_TIME 6\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:6\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 8.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:6\n",
      "0.142857142857 0.04\n",
      "LearningAgent.update(): deadline = 24, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10], [6, 8.5, 24, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "138     9    10.0        26        7\n",
      "139    10    10.0        25        7\n",
      "140    11    10.0        24        7\n",
      "141    12    22.0        23        7\n",
      "142     0     2.0        20        8\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "167     6     8.5        24       10\n",
      "\n",
      "[168 rows x 4 columns]\n",
      "Old_TIME 7\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:7\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 10.5\n",
      "The new state is: ('red', 'left', None, None)\n",
      "t:7\n",
      "0.125 0.0416666666667\n",
      "LearningAgent.update(): deadline = 23, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10], [6, 8.5, 24, 10], [7, 10.5, 23, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "139    10    10.0        25        7\n",
      "140    11    10.0        24        7\n",
      "141    12    22.0        23        7\n",
      "142     0     2.0        20        8\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "167     6     8.5        24       10\n",
      "168     7    10.5        23       10\n",
      "\n",
      "[169 rows x 4 columns]\n",
      "Old_TIME 8\n",
      "The current state is: ('green', 'left', None, None)\n",
      "t:8\n",
      "left\n",
      "REWARD IS: 2.0\n",
      "Total Reward 12.5\n",
      "The new state is: ('green', 'forward', None, None)\n",
      "t:8\n",
      "0.111111111111 0.0434782608696\n",
      "LearningAgent.update(): deadline = 22, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = 2.0\n",
      "New_time 8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10], [6, 8.5, 24, 10], [7, 10.5, 23, 10], [8, 12.5, 22, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "140    11    10.0        24        7\n",
      "141    12    22.0        23        7\n",
      "142     0     2.0        20        8\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "167     6     8.5        24       10\n",
      "168     7    10.5        23       10\n",
      "169     8    12.5        22       10\n",
      "\n",
      "[170 rows x 4 columns]\n",
      "Old_TIME 9\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:9\n",
      "{'forward': -0.8267516503772291, 'right': -0.399061670105989, None: 0.0, 'left': -0.8626886145404664}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 11.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:9\n",
      "0.1 0.0454545454545\n",
      "LearningAgent.update(): deadline = 21, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10], [6, 8.5, 24, 10], [7, 10.5, 23, 10], [8, 12.5, 22, 10], [9, 11.5, 21, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "141    12    22.0        23        7\n",
      "142     0     2.0        20        8\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "167     6     8.5        24       10\n",
      "168     7    10.5        23       10\n",
      "169     8    12.5        22       10\n",
      "170     9    11.5        21       10\n",
      "\n",
      "[171 rows x 4 columns]\n",
      "Old_TIME 10\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:10\n",
      "{'forward': -0.8267516503772291, 'right': -0.399061670105989, None: 0.0, 'left': -0.8764197530864197}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 10.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:10\n",
      "0.0909090909091 0.047619047619\n",
      "LearningAgent.update(): deadline = 20, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10], [6, 8.5, 24, 10], [7, 10.5, 23, 10], [8, 12.5, 22, 10], [9, 11.5, 21, 10], [10, 10.5, 20, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "142     0     2.0        20        8\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "167     6     8.5        24       10\n",
      "168     7    10.5        23       10\n",
      "169     8    12.5        22       10\n",
      "170     9    11.5        21       10\n",
      "171    10    10.5        20       10\n",
      "\n",
      "[172 rows x 4 columns]\n",
      "Old_TIME 11\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:11\n",
      "{'forward': -0.8267516503772291, 'right': -0.399061670105989, None: 0.0, 'left': -0.8876543209876543}\n",
      "forward\n",
      "REWARD IS: -1.0\n",
      "Total Reward 9.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:11\n",
      "0.0833333333333 0.05\n",
      "LearningAgent.update(): deadline = 19, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = -1.0\n",
      "New_time 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10], [6, 8.5, 24, 10], [7, 10.5, 23, 10], [8, 12.5, 22, 10], [9, 11.5, 21, 10], [10, 10.5, 20, 10], [11, 9.5, 19, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "143     1     4.0        19        8\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "167     6     8.5        24       10\n",
      "168     7    10.5        23       10\n",
      "169     8    12.5        22       10\n",
      "170     9    11.5        21       10\n",
      "171    10    10.5        20       10\n",
      "172    11     9.5        19       10\n",
      "\n",
      "[173 rows x 4 columns]\n",
      "Old_TIME 12\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:12\n",
      "forward\n",
      "REWARD IS: 2.0\n",
      "Total Reward 11.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:12\n",
      "0.0769230769231 0.0526315789474\n",
      "LearningAgent.update(): deadline = 18, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 2.0\n",
      "New_time 12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10], [6, 8.5, 24, 10], [7, 10.5, 23, 10], [8, 12.5, 22, 10], [9, 11.5, 21, 10], [10, 10.5, 20, 10], [11, 9.5, 19, 10], [12, 11.5, 18, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "144     2     3.0        18        8\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "167     6     8.5        24       10\n",
      "168     7    10.5        23       10\n",
      "169     8    12.5        22       10\n",
      "170     9    11.5        21       10\n",
      "171    10    10.5        20       10\n",
      "172    11     9.5        19       10\n",
      "173    12    11.5        18       10\n",
      "\n",
      "[174 rows x 4 columns]\n",
      "Old_TIME 13\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:13\n",
      "{'forward': -0.8411890128457933, 'right': -0.399061670105989, None: 0.0, 'left': -0.8876543209876543}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 11.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:13\n",
      "0.0714285714286 0.0555555555556\n",
      "LearningAgent.update(): deadline = 17, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 13\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10], [6, 8.5, 24, 10], [7, 10.5, 23, 10], [8, 12.5, 22, 10], [9, 11.5, 21, 10], [10, 10.5, 20, 10], [11, 9.5, 19, 10], [12, 11.5, 18, 10], [13, 11.5, 17, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "145     3     2.5        17        8\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "167     6     8.5        24       10\n",
      "168     7    10.5        23       10\n",
      "169     8    12.5        22       10\n",
      "170     9    11.5        21       10\n",
      "171    10    10.5        20       10\n",
      "172    11     9.5        19       10\n",
      "173    12    11.5        18       10\n",
      "174    13    11.5        17       10\n",
      "\n",
      "[175 rows x 4 columns]\n",
      "Old_TIME 14\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:14\n",
      "{'forward': -0.8411890128457933, 'right': -0.399061670105989, None: 0.0, 'left': -0.8876543209876543}\n",
      "left\n",
      "REWARD IS: -1.0\n",
      "Total Reward 10.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:14\n",
      "0.0666666666667 0.0588235294118\n",
      "LearningAgent.update(): deadline = 16, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = left, reward = -1.0\n",
      "New_time 14\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10], [6, 8.5, 24, 10], [7, 10.5, 23, 10], [8, 12.5, 22, 10], [9, 11.5, 21, 10], [10, 10.5, 20, 10], [11, 9.5, 19, 10], [12, 11.5, 18, 10], [13, 11.5, 17, 10], [14, 10.5, 16, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "146     4     4.5        16        8\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "167     6     8.5        24       10\n",
      "168     7    10.5        23       10\n",
      "169     8    12.5        22       10\n",
      "170     9    11.5        21       10\n",
      "171    10    10.5        20       10\n",
      "172    11     9.5        19       10\n",
      "173    12    11.5        18       10\n",
      "174    13    11.5        17       10\n",
      "175    14    10.5        16       10\n",
      "\n",
      "[176 rows x 4 columns]\n",
      "Old_TIME 15\n",
      "The current state is: ('red', 'forward', None, None)\n",
      "t:15\n",
      "{'forward': -0.8411890128457933, 'right': -0.399061670105989, None: 0.0, 'left': -0.8951440329218107}\n",
      "None\n",
      "REWARD IS: 0.0\n",
      "Total Reward 10.5\n",
      "The new state is: ('red', 'forward', None, None)\n",
      "t:15\n",
      "0.0625 0.0625\n",
      "LearningAgent.update(): deadline = 15, inputs = {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, action = None, reward = 0.0\n",
      "New_time 15\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10], [6, 8.5, 24, 10], [7, 10.5, 23, 10], [8, 12.5, 22, 10], [9, 11.5, 21, 10], [10, 10.5, 20, 10], [11, 9.5, 19, 10], [12, 11.5, 18, 10], [13, 11.5, 17, 10], [14, 10.5, 16, 10], [15, 10.5, 15, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "147     5     6.5        15        8\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "167     6     8.5        24       10\n",
      "168     7    10.5        23       10\n",
      "169     8    12.5        22       10\n",
      "170     9    11.5        21       10\n",
      "171    10    10.5        20       10\n",
      "172    11     9.5        19       10\n",
      "173    12    11.5        18       10\n",
      "174    13    11.5        17       10\n",
      "175    14    10.5        16       10\n",
      "176    15    10.5        15       10\n",
      "\n",
      "[177 rows x 4 columns]\n",
      "Old_TIME 16\n",
      "The current state is: ('green', 'forward', None, None)\n",
      "t:16\n",
      "RANDOM ACTION\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "REWARD IS: 12.0\n",
      "Total Reward 22.5\n",
      "The new state is: ('red', None, None, None)\n",
      "t:16\n",
      "0.0588235294118 0.0666666666667\n",
      "LearningAgent.update(): deadline = 14, inputs = {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, action = forward, reward = 12.0\n",
      "New_time 16\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0, 2.0, 30, 1], [1, 4.0, 29, 1], [2, 3.0, 28, 1], [3, 5.0, 27, 1], [4, 4.5, 26, 1], [5, 4.0, 25, 1], [6, 3.5, 24, 1], [7, 5.5, 23, 1], [8, 7.5, 22, 1], [9, 7.5, 21, 1], [10, 9.5, 20, 1], [11, 21.5, 19, 1], [0, 2.0, 30, 2], [1, 4.0, 29, 2], [2, 3.0, 28, 2], [3, 2.5, 27, 2], [4, 2.5, 26, 2], [5, 2.0, 25, 2], [6, 4.0, 24, 2], [7, 4.0, 23, 2], [8, 3.0, 22, 2], [9, 3.0, 21, 2], [10, 5.0, 20, 2], [11, 4.0, 19, 2], [12, 6.0, 18, 2], [13, 5.0, 17, 2], [14, 4.5, 16, 2], [15, 6.5, 15, 2], [16, 5.5, 14, 2], [17, 5.0, 13, 2], [18, 7.0, 12, 2], [19, 9.0, 11, 2], [20, 11.0, 10, 2], [21, 10.0, 9, 2], [22, 9.0, 8, 2], [23, 8.0, 7, 2], [24, 10.0, 6, 2], [25, 12.0, 5, 2], [26, 11.0, 4, 2], [27, 10.0, 3, 2], [28, 22.0, 2, 2], [0, 2.0, 35, 3], [1, 4.0, 34, 3], [2, 3.5, 33, 3], [3, 2.5, 32, 3], [4, 2.0, 31, 3], [5, 4.0, 30, 3], [6, 6.0, 29, 3], [7, 5.0, 28, 3], [8, 4.0, 27, 3], [9, 6.0, 26, 3], [10, 5.5, 25, 3], [11, 4.5, 24, 3], [12, 6.5, 23, 3], [13, 8.5, 22, 3], [14, 7.5, 21, 3], [15, 7.5, 20, 3], [16, 9.5, 19, 3], [17, 9.0, 18, 3], [18, 9.0, 17, 3], [19, 11.0, 16, 3], [20, 13.0, 15, 3], [21, 15.0, 14, 3], [22, 17.0, 13, 3], [23, 16.5, 12, 3], [24, 18.5, 11, 3], [25, 20.5, 10, 3], [26, 22.5, 9, 3], [27, 34.5, 8, 3], [0, 2.0, 30, 4], [1, 1.5, 29, 4], [2, 1.0, 28, 4], [3, 3.0, 27, 4], [4, 5.0, 26, 4], [5, 7.0, 25, 4], [6, 9.0, 24, 4], [7, 11.0, 23, 4], [8, 13.0, 22, 4], [9, 12.5, 21, 4], [10, 14.5, 20, 4], [11, 16.5, 19, 4], [12, 18.5, 18, 4], [13, 18.0, 17, 4], [14, 20.0, 16, 4], [15, 32.0, 15, 4], [0, 2.0, 45, 5], [1, 1.0, 44, 5], [2, 0.0, 43, 5], [3, 2.0, 42, 5], [4, 1.5, 41, 5], [5, 3.5, 40, 5], [6, 5.5, 39, 5], [7, 4.5, 38, 5], [8, 3.5, 37, 5], [9, 5.5, 36, 5], [10, 5.5, 35, 5], [11, 4.5, 34, 5], [12, 4.5, 33, 5], [13, 4.5, 32, 5], [14, 4.5, 31, 5], [15, 6.5, 30, 5], [16, 8.5, 29, 5], [17, 7.5, 28, 5], [18, 9.5, 27, 5], [19, 8.5, 26, 5], [20, 10.5, 25, 5], [21, 22.5, 24, 5], [0, 0.0, 30, 6], [1, 2.0, 29, 6], [2, 1.0, 28, 6], [3, 3.0, 27, 6], [4, 2.0, 26, 6], [5, 1.0, 25, 6], [6, 0.5, 24, 6], [7, 2.5, 23, 6], [8, 2.5, 22, 6], [9, 4.5, 21, 6], [10, 4.5, 20, 6], [11, 6.5, 19, 6], [12, 8.5, 18, 6], [13, 8.0, 17, 6], [14, 10.0, 16, 6], [15, 12.0, 15, 6], [16, 14.0, 14, 6], [17, 13.5, 13, 6], [18, 15.5, 12, 6], [19, 17.5, 11, 6], [20, 19.5, 10, 6], [21, 31.5, 9, 6], [0, 2.0, 35, 7], [1, 4.0, 34, 7], [2, 6.0, 33, 7], [3, 8.0, 32, 7], [4, 7.5, 31, 7], [5, 9.5, 30, 7], [6, 8.5, 29, 7], [7, 8.5, 28, 7], [8, 8.0, 27, 7], [9, 10.0, 26, 7], [10, 10.0, 25, 7], [11, 10.0, 24, 7], [12, 22.0, 23, 7], [0, 2.0, 20, 8], [1, 4.0, 19, 8], [2, 3.0, 18, 8], [3, 2.5, 17, 8], [4, 4.5, 16, 8], [5, 6.5, 15, 8], [6, 8.5, 14, 8], [7, 10.5, 13, 8], [8, 22.5, 12, 8], [0, 2.0, 30, 9], [1, 4.0, 29, 9], [2, 6.0, 28, 9], [3, 5.0, 27, 9], [4, 4.0, 26, 9], [5, 6.0, 25, 9], [6, 6.0, 24, 9], [7, 5.0, 23, 9], [8, 7.0, 22, 9], [9, 19.0, 21, 9], [0, 2.0, 30, 10], [1, 1.5, 29, 10], [2, 3.5, 28, 10], [3, 5.5, 27, 10], [4, 7.5, 26, 10], [5, 6.5, 25, 10], [6, 8.5, 24, 10], [7, 10.5, 23, 10], [8, 12.5, 22, 10], [9, 11.5, 21, 10], [10, 10.5, 20, 10], [11, 9.5, 19, 10], [12, 11.5, 18, 10], [13, 11.5, 17, 10], [14, 10.5, 16, 10], [15, 10.5, 15, 10], [16, 22.5, 14, 10]]\n",
      "     Time  Reward  Deadline  Episode\n",
      "0       0     2.0        30        1\n",
      "1       1     4.0        29        1\n",
      "2       2     3.0        28        1\n",
      "3       3     5.0        27        1\n",
      "4       4     4.5        26        1\n",
      "5       5     4.0        25        1\n",
      "6       6     3.5        24        1\n",
      "7       7     5.5        23        1\n",
      "8       8     7.5        22        1\n",
      "9       9     7.5        21        1\n",
      "10     10     9.5        20        1\n",
      "11     11    21.5        19        1\n",
      "12      0     2.0        30        2\n",
      "13      1     4.0        29        2\n",
      "14      2     3.0        28        2\n",
      "15      3     2.5        27        2\n",
      "16      4     2.5        26        2\n",
      "17      5     2.0        25        2\n",
      "18      6     4.0        24        2\n",
      "19      7     4.0        23        2\n",
      "20      8     3.0        22        2\n",
      "21      9     3.0        21        2\n",
      "22     10     5.0        20        2\n",
      "23     11     4.0        19        2\n",
      "24     12     6.0        18        2\n",
      "25     13     5.0        17        2\n",
      "26     14     4.5        16        2\n",
      "27     15     6.5        15        2\n",
      "28     16     5.5        14        2\n",
      "29     17     5.0        13        2\n",
      "..    ...     ...       ...      ...\n",
      "148     6     8.5        14        8\n",
      "149     7    10.5        13        8\n",
      "150     8    22.5        12        8\n",
      "151     0     2.0        30        9\n",
      "152     1     4.0        29        9\n",
      "153     2     6.0        28        9\n",
      "154     3     5.0        27        9\n",
      "155     4     4.0        26        9\n",
      "156     5     6.0        25        9\n",
      "157     6     6.0        24        9\n",
      "158     7     5.0        23        9\n",
      "159     8     7.0        22        9\n",
      "160     9    19.0        21        9\n",
      "161     0     2.0        30       10\n",
      "162     1     1.5        29       10\n",
      "163     2     3.5        28       10\n",
      "164     3     5.5        27       10\n",
      "165     4     7.5        26       10\n",
      "166     5     6.5        25       10\n",
      "167     6     8.5        24       10\n",
      "168     7    10.5        23       10\n",
      "169     8    12.5        22       10\n",
      "170     9    11.5        21       10\n",
      "171    10    10.5        20       10\n",
      "172    11     9.5        19       10\n",
      "173    12    11.5        18       10\n",
      "174    13    11.5        17       10\n",
      "175    14    10.5        16       10\n",
      "176    15    10.5        15       10\n",
      "177    16    22.5        14       10\n",
      "\n",
      "[178 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from environment import Agent, Environment\n",
    "from planner import RoutePlanner\n",
    "from simulator import Simulator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class LearningAgent(Agent):\n",
    "    \"\"\"An agent that learns to drive in the smartcab world.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(LearningAgent, self).__init__(env)  # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'red'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        # TODO: Initialize any additional variables here\n",
    "        \n",
    "        #Set traffic_light and movement\n",
    "        traffic_light=[\"red\",\"green\"]\n",
    "        motion = [None, 'forward', 'left', 'right']\n",
    "        waypoint,oncoming,left=motion,motion,motion\n",
    "\n",
    "        \n",
    "        #Initialize q_table\n",
    "        #Set all features to 0\n",
    "        self.q_table = {}\n",
    "        for light in traffic_light:\n",
    "            for point in waypoint:\n",
    "                for on in oncoming:\n",
    "                    for lf in left:\n",
    "                        self.q_table[(light, point, on, lf)] = {None: 0, 'forward': 0, 'left': 0, 'right': 0}\n",
    "\n",
    "        print self.q_table\n",
    "\n",
    "\n",
    "        self.episode=0\n",
    "        self.preserve=[]\n",
    "\n",
    "    def reset(self,destination=None,total=0):\n",
    "\n",
    "        self.planner.route_to(destination)\n",
    "        # TODO: Prepare for a new trip; reset any variables here, if required\n",
    "        self.total_reward=total\n",
    "\n",
    "        \n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        deadline = self.env.get_deadline(self)\n",
    "        \n",
    "        \n",
    "        old_time=t\n",
    "        print 'Old_TIME',old_time\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        # TODO: Update state\n",
    "        self.state = (inputs['light'],\n",
    "                      self.next_waypoint,\n",
    "                      inputs['oncoming'],\n",
    "                      inputs['left'])\n",
    "\n",
    "        print \"The current state is: {}\".format(self.state)\n",
    "\n",
    "        print \"t:{}\".format(t)\n",
    "        \n",
    "        # TODO: Select action according to your policy\n",
    "        epsilon=0.1\n",
    "        rand=random.random()\n",
    "        if rand<epsilon:\n",
    "            action=random.choice(Environment.valid_actions[0:])\n",
    "            print \"RANDOM ACTION\"\n",
    "        else:\n",
    "            if max(self.q_table[self.state].values())==0:\n",
    "                action=random.choice(Environment.valid_actions[0:])\n",
    "                print self.q_table[self.state]\n",
    "               \n",
    "            else:\n",
    "                action = max(self.q_table[self.state],\n",
    "                     key=self.q_table[self.state].get)\n",
    "\n",
    "            print action\n",
    "\n",
    "        # Execute action and get reward\n",
    "        reward = self.env.act(self, action)\n",
    "        \n",
    "        print \"REWARD IS:\",reward\n",
    "        self.total_reward+=reward\n",
    "     \n",
    "        print \"Total Reward\",self.total_reward\n",
    "        \n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "        # Set the tuning parameters\n",
    "        alpha = 1.0/(1.0+t) # learning rate\n",
    "        gamma = 1.0/(1.0+deadline) # discount factor\n",
    "      #  alpha=0.5\n",
    "       # gamma=0.2\n",
    "        # Get the new state after the above action\n",
    "\n",
    "        inputs_new = self.env.sense(self)\n",
    "        state_new = (inputs_new['light'],\n",
    "                     self.planner.next_waypoint(),\n",
    "                     inputs_new['oncoming'],\n",
    "                     inputs_new['left'])\n",
    "        print \"The new state is: {}\".format(state_new)\n",
    "        print \"t:{}\".format(t)\n",
    "        print alpha,gamma\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Calculate the Q_value\n",
    "        q_value = (1 - alpha) * self.q_table[self.state][action] + \\\n",
    "                  alpha * (reward + gamma * max(self.q_table[state_new].values()))\n",
    "        # Update the Q_table\n",
    "        self.q_table[self.state][action] = q_value\n",
    "        # Set current state and action as previous state and action\n",
    "        \n",
    "        \n",
    "        print \"LearningAgent.update(): deadline = {}, inputs = {}, action = {}, reward = {}\".format(deadline, inputs, action, reward)  # [debug]\n",
    "        new_time=t\n",
    "        print 'New_time',new_time\n",
    "        total_preserve=self.total_reward\n",
    "        print \"\\n\"\n",
    "        print \"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "        if old_time==0:\n",
    "            self.episode+=1\n",
    "\n",
    "        self.preserve.append([new_time,self.total_reward,deadline,self.episode])\n",
    "      #  self.preserve.append(new_time)\n",
    "      #  self.preserve.append(self.total_reward)\n",
    "      #  self.preserve.append(self.episode)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        if self.episode==10:\n",
    "            df1=pd.DataFrame(self.preserve,columns=['Time','Reward','Deadline','Episode'])\n",
    "            print self.preserve\n",
    "            print df1\n",
    "            return df1\n",
    "       \n",
    "        \n",
    "    \n",
    "def run():\n",
    "    \"\"\"Run the agent for a finite number of trials.\"\"\"\n",
    "    # Set up environment and agent\n",
    "    e = Environment()  # create environment (also adds some dummy traffic)\n",
    "    a = e.create_agent(LearningAgent)  # create agent\n",
    "    e.set_primary_agent(a, enforce_deadline=True)  # set agent to track\n",
    "    # Now simulate it\n",
    "    sim = Simulator(e, update_delay=0.1,display=True)  # reduce update_delay to speed up simulation\n",
    "    sim.run(n_trials=10)  # press Esc or close pygame window to quit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=[[0, 2.0, 30, 1], [1, 1.5, 29, 1], [2, 3.5, 28, 1], [3, 3.0, 27, 1], [4, 3.0, 26, 1], [5, 5.0, 25, 1], [6, 5.0, 24, 1], [7, 5.0, 23, 1], [8, 4.5, 22, 1], [9, 4.0, 21, 1], [10, 6.0, 20, 1], [11, 8.0, 19, 1], [12, 7.5, 18, 1], [13, 9.5, 17, 1], [14, 8.5, 16, 1], [15, 8.0, 15, 1], [16, 10.0, 14, 1], [17, 12.0, 13, 1], [18, 14.0, 12, 1], [19, 14.0, 11, 1], [20, 13.5, 10, 1], [21, 13.5, 9, 1], [22, 13.5, 8, 1], [23, 15.5, 7, 1], [24, 17.5, 6, 1], [25, 19.5, 5, 1], [26, 18.5, 4, 1], [27, 17.5, 3, 1], [28, 16.5, 2, 1], [29, 16.0, 1, 1], [30, 18.0, 0, 1], [0, 2.0, 40, 2], [1, 4.0, 39, 2], [2, 3.0, 38, 2], [3, 3.0, 37, 2], [4, 5.0, 36, 2], [5, 14.5, 35, 2], [0, -1.0, 25, 3], [1, -2.0, 24, 3], [2, -3.0, 23, 3], [3, -1.0, 22, 3], [4, -2.0, 21, 3], [5, -2.0, 20, 3], [6, -2.5, 19, 3], [7, -3.0, 18, 3], [8, -1.0, 17, 3], [9, 1.0, 16, 3], [10, 0.5, 15, 3], [11, 2.5, 14, 3], [12, 2.0, 13, 3], [13, 4.0, 12, 3], [14, 6.0, 11, 3], [15, 8.0, 10, 3], [16, 10.0, 9, 3], [17, 12.0, 8, 3], [18, 11.5, 7, 3], [19, 13.5, 6, 3], [20, 13.5, 5, 3], [21, 12.5, 4, 3], [22, 14.5, 3, 3], [23, 16.5, 2, 3], [24, 15.5, 1, 3], [25, 14.5, 0, 3], [0, 2.0, 35, 4], [1, 2.0, 34, 4], [2, 1.5, 33, 4], [3, 3.5, 32, 4], [4, 3.0, 31, 4], [5, 5.0, 30, 4], [6, 7.0, 29, 4], [7, 7.0, 28, 4], [8, 7.0, 27, 4], [9, 6.0, 26, 4], [10, 18.0, 25, 4], [0, -0.5, 30, 5], [1, 1.5, 29, 5], [2, 3.5, 28, 5], [3, 3.0, 27, 5], [4, 2.5, 26, 5], [5, 4.5, 25, 5], [6, 6.5, 24, 5], [7, 6.0, 23, 5], [8, 8.0, 22, 5], [9, 10.0, 21, 5], [10, 12.0, 20, 5], [11, 11.0, 19, 5], [12, 10.0, 18, 5], [13, 9.5, 17, 5], [14, 8.5, 16, 5], [15, 8.5, 15, 5], [16, 10.5, 14, 5], [17, 9.5, 13, 5], [18, 8.5, 12, 5], [19, 8.5, 11, 5], [20, 20.5, 10, 5], [0, 2.0, 20, 6], [1, 4.0, 19, 6], [2, 3.5, 18, 6], [3, 5.5, 17, 6], [4, 7.5, 16, 6], [5, 9.5, 15, 6], [6, 11.5, 14, 6], [7, 23.5, 13, 6], [0, 2.0, 20, 7], [1, 2.0, 19, 7], [2, 1.0, 18, 7], [3, 0.0, 17, 7], [4, 2.0, 16, 7], [5, 2.0, 15, 7], [6, 2.0, 14, 7], [7, 2.0, 13, 7], [8, 4.0, 12, 7], [9, 4.0, 11, 7], [10, 3.0, 10, 7], [11, 2.5, 9, 7], [12, 2.0, 8, 7], [13, 4.0, 7, 7], [14, 6.0, 6, 7], [15, 18.0, 5, 7], [0, -0.5, 25, 8], [1, -1.5, 24, 8], [2, -2.5, 23, 8], [3, -3.5, 22, 8], [4, -3.5, 21, 8], [5, -1.5, 20, 8], [6, -2.5, 19, 8], [7, -2.5, 18, 8], [8, -2.5, 17, 8], [9, -3.5, 16, 8], [10, -1.5, 15, 8], [11, -2.5, 14, 8], [12, -0.5, 13, 8], [13, -0.5, 12, 8], [14, -1.5, 11, 8], [15, 10.5, 10, 8], [0, 2.0, 20, 9], [1, 1.0, 19, 9], [2, 0.5, 18, 9], [3, 0.0, 17, 9], [4, -0.5, 16, 9], [5, -1.5, 15, 9], [6, -2.5, 14, 9], [7, -3.5, 13, 9], [8, -4.5, 12, 9], [9, -5.0, 11, 9], [10, -5.0, 10, 9], [11, -3.0, 9, 9], [12, -1.0, 8, 9], [13, -2.0, 7, 9], [14, -3.0, 6, 9], [15, -4.0, 5, 9], [16, -2.0, 4, 9], [17, 0.0, 3, 9], [18, -1.0, 2, 9], [19, -2.0, 1, 9], [20, -2.0, 0, 9], [0, 2.0, 25, 10], [1, 4.0, 24, 10], [2, 6.0, 23, 10], [3, 8.0, 22, 10], [4, 7.5, 21, 10], [5, 9.5, 20, 10], [6, 11.5, 19, 10], [7, 13.5, 18, 10], [8, 12.5, 17, 10], [9, 12.5, 16, 10], [10, 12.0, 15, 10], [11, 14.0, 14, 10], [12, 16.0, 13, 10], [13, 18.0, 12, 10], [14, 17.5, 11, 10], [15, 17.0, 10, 10], [16, 19.0, 9, 10], [17, 18.0, 8, 10], [18, 20.0, 7, 10], [19, 22.0, 6, 10], [20, 21.0, 5, 10], [21, 20.0, 4, 10], [22, 19.0, 3, 10], [23, 19.0, 2, 10], [24, 19.0, 1, 10], [25, 18.5, 0, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=[[0, 2.0, 30, 1], [1, 1.5, 29, 1], [2, 3.5, 28, 1], [3, 3.0, 27, 1], [4, 3.0, 26, 1], [5, 5.0, 25, 1], [6, 5.0, 24, 1], [7, 5.0, 23, 1], [8, 4.5, 22, 1], [9, 4.0, 21, 1], [10, 6.0, 20, 1], [11, 8.0, 19, 1], [12, 7.5, 18, 1], [13, 9.5, 17, 1], [14, 8.5, 16, 1], [15, 8.0, 15, 1], [16, 10.0, 14, 1], [17, 12.0, 13, 1], [18, 14.0, 12, 1], [19, 14.0, 11, 1], [20, 13.5, 10, 1], [21, 13.5, 9, 1], [22, 13.5, 8, 1], [23, 15.5, 7, 1], [24, 17.5, 6, 1], [25, 19.5, 5, 1], [26, 18.5, 4, 1], [27, 17.5, 3, 1], [28, 16.5, 2, 1], [29, 16.0, 1, 1], [30, 18.0, 0, 1], [0, 2.0, 40, 2], [1, 4.0, 39, 2], [2, 3.0, 38, 2], [3, 3.0, 37, 2], [4, 5.0, 36, 2], [5, 14.5, 35, 2], [0, -1.0, 25, 3], [1, -2.0, 24, 3], [2, -3.0, 23, 3], [3, -1.0, 22, 3], [4, -2.0, 21, 3], [5, -2.0, 20, 3], [6, -2.5, 19, 3], [7, -3.0, 18, 3], [8, -1.0, 17, 3], [9, 1.0, 16, 3], [10, 0.5, 15, 3], [11, 2.5, 14, 3], [12, 2.0, 13, 3], [13, 4.0, 12, 3], [14, 6.0, 11, 3], [15, 8.0, 10, 3], [16, 10.0, 9, 3], [17, 12.0, 8, 3], [18, 11.5, 7, 3], [19, 13.5, 6, 3], [20, 13.5, 5, 3], [21, 12.5, 4, 3], [22, 14.5, 3, 3], [23, 16.5, 2, 3], [24, 15.5, 1, 3], [25, 14.5, 0, 3], [0, 2.0, 35, 4], [1, 2.0, 34, 4], [2, 1.5, 33, 4], [3, 3.5, 32, 4], [4, 3.0, 31, 4], [5, 5.0, 30, 4], [6, 7.0, 29, 4], [7, 7.0, 28, 4], [8, 7.0, 27, 4], [9, 6.0, 26, 4], [10, 18.0, 25, 4], [0, -0.5, 30, 5], [1, 1.5, 29, 5], [2, 3.5, 28, 5], [3, 3.0, 27, 5], [4, 2.5, 26, 5], [5, 4.5, 25, 5], [6, 6.5, 24, 5], [7, 6.0, 23, 5], [8, 8.0, 22, 5], [9, 10.0, 21, 5], [10, 12.0, 20, 5], [11, 11.0, 19, 5], [12, 10.0, 18, 5], [13, 9.5, 17, 5], [14, 8.5, 16, 5], [15, 8.5, 15, 5], [16, 10.5, 14, 5], [17, 9.5, 13, 5], [18, 8.5, 12, 5], [19, 8.5, 11, 5], [20, 20.5, 10, 5], [0, 2.0, 20, 6], [1, 4.0, 19, 6], [2, 3.5, 18, 6], [3, 5.5, 17, 6], [4, 7.5, 16, 6], [5, 9.5, 15, 6], [6, 11.5, 14, 6], [7, 23.5, 13, 6], [0, 2.0, 20, 7], [1, 2.0, 19, 7], [2, 1.0, 18, 7], [3, 0.0, 17, 7], [4, 2.0, 16, 7], [5, 2.0, 15, 7], [6, 2.0, 14, 7], [7, 2.0, 13, 7], [8, 4.0, 12, 7], [9, 4.0, 11, 7], [10, 3.0, 10, 7], [11, 2.5, 9, 7], [12, 2.0, 8, 7], [13, 4.0, 7, 7], [14, 6.0, 6, 7], [15, 18.0, 5, 7], [0, -0.5, 25, 8], [1, -1.5, 24, 8], [2, -2.5, 23, 8], [3, -3.5, 22, 8], [4, -3.5, 21, 8], [5, -1.5, 20, 8], [6, -2.5, 19, 8], [7, -2.5, 18, 8], [8, -2.5, 17, 8], [9, -3.5, 16, 8], [10, -1.5, 15, 8], [11, -2.5, 14, 8], [12, -0.5, 13, 8], [13, -0.5, 12, 8], [14, -1.5, 11, 8], [15, 10.5, 10, 8], [0, 2.0, 20, 9], [1, 1.0, 19, 9], [2, 0.5, 18, 9], [3, 0.0, 17, 9], [4, -0.5, 16, 9], [5, -1.5, 15, 9], [6, -2.5, 14, 9], [7, -3.5, 13, 9], [8, -4.5, 12, 9], [9, -5.0, 11, 9], [10, -5.0, 10, 9], [11, -3.0, 9, 9], [12, -1.0, 8, 9], [13, -2.0, 7, 9], [14, -3.0, 6, 9], [15, -4.0, 5, 9], [16, -2.0, 4, 9], [17, 0.0, 3, 9], [18, -1.0, 2, 9], [19, -2.0, 1, 9], [20, -2.0, 0, 9], [0, 2.0, 25, 10], [1, 4.0, 24, 10], [2, 6.0, 23, 10], [3, 8.0, 22, 10], [4, 7.5, 21, 10], [5, 9.5, 20, 10], [6, 11.5, 19, 10], [7, 13.5, 18, 10], [8, 12.5, 17, 10], [9, 12.5, 16, 10], [10, 12.0, 15, 10], [11, 14.0, 14, 10], [12, 16.0, 13, 10], [13, 18.0, 12, 10], [14, 17.5, 11, 10], [15, 17.0, 10, 10], [16, 19.0, 9, 10], [17, 18.0, 8, 10], [18, 20.0, 7, 10], [19, 22.0, 6, 10], [20, 21.0, 5, 10], [21, 20.0, 4, 10], [22, 19.0, 3, 10], [23, 19.0, 2, 10], [24, 19.0, 1, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(b,columns=['Time','Reward','Deadline','Episode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e51f5dd68a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'deadline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-6906b1e4a953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mLearningAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeadline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'deadline'"
     ]
    }
   ],
   "source": [
    "print LearningAgent.update.deadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
